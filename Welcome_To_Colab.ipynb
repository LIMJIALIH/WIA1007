{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LIMJIALIH/WIA1007/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Selection\n",
        "Once your data is ready, you will need to choose an appropriate machine learning algorithm for your project. You may choose from supervised or unsupervised learning, and select the model based on the problem you are trying to solve."
      ],
      "metadata": {
        "id": "XUzbl16CIDzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start with our machine learning, we need to make a copy of our dataset to avoid modifiying the original"
      ],
      "metadata": {
        "id": "e0QqSL3wIfRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = scaled_df.copy()"
      ],
      "metadata": {
        "id": "Y-N1smd6JwvQ",
        "outputId": "8b559d22-e38c-46b8-bc72-f48a7388cb99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'scaled_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a52384aadeb4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scaled_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.isna().sum()"
      ],
      "metadata": {
        "id": "DTQY3We5KAno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head()"
      ],
      "metadata": {
        "id": "pfYUF0pNKCiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.info()"
      ],
      "metadata": {
        "id": "kXsmfOxuKGe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = final_df.select_dtypes(include=['number']).corr()\n",
        "\n",
        "# Plot the correlation matrix as a heatmap\n",
        "plt.figure(figsize=(12, 10))  # Adjust figure size as needed\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lSoWPsPZKhfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###There are four categorical data location, property type, furnishing and size category.\n",
        "\n",
        "### All we make them into True and False, and then drop all these categorical data."
      ],
      "metadata": {
        "id": "8F4vvvi9KwKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your DataFrame is named 'scaled_df'\n",
        "\n",
        "# Get a list of categorical features\n",
        "categorical_features = ['Location', 'Property Type', 'Furnishing', 'Size Category']\n",
        "\n",
        "# Perform one-hot encoding\n",
        "encoded_df = pd.get_dummies(scaled_df, columns=categorical_features, drop_first=True, dtype=int)\n",
        "# dtype=int ensures the encoded columns are integers (True/False represented as 1/0)\n",
        "# drop_first=True avoids multicollinearity by dropping one level from each categorical feature\n",
        "\n",
        "# Drop the original categorical columns\n",
        "final_df = encoded_df.drop(categorical_features, axis=1, errors='ignore')\n",
        "# errors='ignore' to handle cases where a column might have already been dropped\n",
        "\n",
        "# Display the final DataFrame\n",
        "print(final_df.head())"
      ],
      "metadata": {
        "id": "Y2eCSf3lK297"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export the df_cleaned to 'balanced_cleaned_credit_card_data.csv'\n",
        "\n",
        "final_df.to_csv('cleaned_data_for_ml.csv', index=False)"
      ],
      "metadata": {
        "id": "frAbHDrPLcgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Train and Test Data\n",
        "\n",
        "Like every Machine Learning training steps, we will start with a train-test split on our data."
      ],
      "metadata": {
        "id": "guNiW_5zLjGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Read the preprocessed and cleaned dataset\n",
        "# The 'Price' column is not the first column, so no need to specify index_col\n",
        "X = pd.read_csv('cleaned_data_for_ml.csv')\n",
        "\n",
        "#Extract the target variable ('Price') and set it as variable 'y'\n",
        "y = X.pop('Price')  # Now 'Price' is accessible as a regular column\n",
        "\n",
        "#Split dataset into training and testing sets\n",
        "#75% data used for training, 25% used for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "mY1NaZ4pLrgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have split our dataset into training and testing sets, where the Train Data takes a random 75% of the dataset and the Test Data consists of the remaining 25% of the dataset. Our model will perform training on the Train Data and perform model evaluation on the Test Data to determine the better model."
      ],
      "metadata": {
        "id": "fSiSQF4MLxGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determining Best Model"
      ],
      "metadata": {
        "id": "wBMAVpWbMeIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a lot of different classification models to test and see which model performs the best on our data. We have decided to use the R-squared and Root Mean Squared Error (RMSE) to measure our model performance."
      ],
      "metadata": {
        "id": "KK9D6oCKMpvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, mean_squared_error"
      ],
      "metadata": {
        "id": "NHAvO-KrM9-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will train our data against multiple classification models to test which gives us the best performance.\n",
        "\n",
        "Some common classification models are as below:\n",
        "\n",
        "- Linear Regression\n",
        "- Decision Tree Classifier\n",
        "- Random Forest Classifier\n",
        "- XGBRegressor\n",
        "\n",
        "Below is the code to train our data against different classification models to help us find out which classification models perform best based on the 'R-squared' and 'Root-mean-square deviation(RMSE)' calculated."
      ],
      "metadata": {
        "id": "fePn5egMN5zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all the necessary libraries for machine learning regression models\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor  # Assumes you have installed xgboost (pip install xgboost)\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a list of regression models\n",
        "regression_model_class = [\n",
        "    LinearRegression,\n",
        "    DecisionTreeRegressor,\n",
        "    RandomForestRegressor,\n",
        "    XGBRegressor,\n",
        "]\n",
        "\n",
        "# Model performance dictionary to store the results\n",
        "model_performance = {\n",
        "    'ModelName': ['LinearRegression',\n",
        "                  'DecisionTreeRegressor',\n",
        "                  'RandomForestRegressor',\n",
        "                  'XGBRegressor'],\n",
        "    'Model': [],\n",
        "    'R-squared': [],  # Use R-squared for regression\n",
        "    'RMSE': [],\n",
        "}\n",
        "\n",
        "# Loop through each regression model class\n",
        "for ModelClass in regression_model_class:\n",
        "    # Create model with specified arguments\n",
        "    args = {'random_state': 161223} if ModelClass != LinearRegression else {}  # Handle XGBoost case\n",
        "    model = ModelClass(**args)\n",
        "\n",
        "    # Train data on model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Model prediction using the trained model\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluation scoring\n",
        "    model_performance['Model'].append(model)\n",
        "    model_performance['R-squared'].append(r2_score(y_test, y_pred))  # Calculate R-squared\n",
        "    model_performance['RMSE'].append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "\n",
        "# Create DataFrame from the model performance dictionary\n",
        "model_performance_df = pd.DataFrame(model_performance)\n",
        "print(model_performance_df)"
      ],
      "metadata": {
        "id": "hGt8TQ1aNeng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Arrange the models from lowest to highest R-squared for better visualization.\n",
        "model_performance_df.sort_values(by='R-squared', inplace=True)\n",
        "\n",
        "# Create a figure and axis for the table\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Display the table in ascending order of R-squared\n",
        "table_data = model_performance_df[['ModelName', 'R-squared', 'RMSE']]\n",
        "table = ax.table(cellText=table_data.values, colLabels=table_data.columns, cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "\n",
        "# Remove axes for better visualization\n",
        "ax.axis('off')\n",
        "\n",
        "plt.title('Model Performance Table')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BzIukcFhPpQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the table, we can see that Random Forest Regressor has the highest R-squared among all the model"
      ],
      "metadata": {
        "id": "2zXc-fVkP328"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the model performance using bar charts. Model performs best if their:\n",
        "\n",
        "- R-squared is highest\n",
        "- RMSE value is lowest\n",
        "\n",
        "The code provided below is determine which model performs the best based on the 'R-squared' value and RMSE value. After performing the code above, the DataFrame now contains the model names, the actual model instances, R-squared scores, and RMSE values for each model. This makes it easier to analyze and visualize the model performance into charts as provided in the code below."
      ],
      "metadata": {
        "id": "jZ9C25dHQjXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize model performa nce by creating two subplots ax1 and ax2\n",
        "fig, (ax1, ax2) = plt.subplots(2, layout='constrained')\n",
        "\n",
        "#Arrange the models from lowest to highest R-squared for better visualization.\n",
        "model_performance_df.sort_values(by='R-squared', inplace=True)\n",
        "#Horizontal bar chart is created on the first subplot (ax1). ( y-axis: 'ModelName', x-axis : 'R-squared')\n",
        "ax1.barh(model_performance_df['ModelName'], model_performance_df['R-squared'], color=sns.color_palette('flare'))\n",
        "#Set title of subplot ax1\n",
        "ax1.set_title('R-squared')\n",
        "\n",
        "#Arrange the models from highest to lowest RMSE for better visualization.\n",
        "model_performance_df.sort_values(by='RMSE', ascending=False, inplace=True)\n",
        "#Horizontal bar chart is created on the second subplot (ax2). ( y-axis: 'ModelName', x-axis : 'RMSE')\n",
        "ax2.barh(model_performance_df['ModelName'], model_performance_df['RMSE'], color=sns.color_palette('flare'))\n",
        "#Set title of subplot ax2\n",
        "ax2.set_title('RMSE')"
      ],
      "metadata": {
        "id": "mS8wSVxDRJeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selected Model"
      ],
      "metadata": {
        "id": "7g68dW6rRn1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the R-squared for RandomForestRegressor is the highest and the RMSE value is the lowest, RandomForestRegressor is the best performing model. Without further ado, We will choose RandomForestRegressor for our model."
      ],
      "metadata": {
        "id": "iR79mrnbRqam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Evaluation"
      ],
      "metadata": {
        "id": "fjQcweHrR7cE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is selected, it needs to be trained on the preprocessed data, and its performance should be evaluated using appropriate metrics. The model can be fine-tuned to improve its performance through Hyperparameter Tuning."
      ],
      "metadata": {
        "id": "NGzn5aw7R9pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "bwyjSnPPSTdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'max_depth': range(1, 10),\n",
        "}\n",
        "\n",
        "#Create an instance of GridSearchCV with RandomForestRegressor, searching over the specified parameter grid\n",
        "#Using 4-fold cross-validation and R-squared as the scoring metric\n",
        "clf = GridSearchCV(RandomForestRegressor(), param_grid=params, scoring='r2', cv=4)\n",
        "\n",
        "# Fit the GridSearchCV instance on the training data to find the best hyperparameters\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Display the best hyperparameters and corresponding best R-squared score\n",
        "clf.best_params_, clf.best_score_"
      ],
      "metadata": {
        "id": "F7n2EpzhSYGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first hyperparameter values found were {'max_depth': 9} with a corresponding best accuracy score of approximately 0.999986."
      ],
      "metadata": {
        "id": "zirv4HlcSfFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'max_depth': [9],\n",
        "    'min_samples_split': range(2, 11),\n",
        "    'min_samples_leaf': [i for i in range(1, 10)],\n",
        "    'ccp_alpha': [i/10.0 for i in range(0, 10)]\n",
        "}\n",
        "\n",
        "#Create an instance of GridSearchCV with RandomForestRegressor, searching over the specified parameter grid\n",
        "#Using 4-fold cross-validation and R-squared as the scoring metric\n",
        "clf = GridSearchCV(RandomForestRegressor(), param_grid=params, scoring='r2', cv=4)\n",
        "# Fit the GridSearchCV instance on the training data to find the best hyperparameters\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Display the best hyperparameters and corresponding best R-squared score\n",
        "clf.best_params_, clf.best_score_"
      ],
      "metadata": {
        "id": "__gShi5aSmF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can extend the GridSearch to include additional hyperparameters: 'max_depth','min_samples_split', 'min_samples_leaf', and 'ccp_alpha'.\n",
        "\n",
        "The best hyperparameter values found in this case were {'ccp_alpha': 0.0, 'max_depth': 9, 'min_samples_leaf': 1, 'min_samples_split': 7} with a corresponding best accuracy score of approximately 0.999991."
      ],
      "metadata": {
        "id": "CSNU-ejlSzSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "wvz_O19cTt0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best hyperparameters found during the tuning process are utilized for training the random forest model as shown above."
      ],
      "metadata": {
        "id": "n2VFZ8hATz8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimenting"
      ],
      "metadata": {
        "id": "bD36ys43Ue7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a df copy and try a different normalisation technique to observe the change in accuracy"
      ],
      "metadata": {
        "id": "dRhzb9cuUjMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_normStandardScaler = scaled_df.copy()"
      ],
      "metadata": {
        "id": "ZnZFLykQWXfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we choose standard scaler as another method to normalise the data as our previous normalised data is using min max scaler"
      ],
      "metadata": {
        "id": "Uj1O5IqPWoTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to change the columns to standardize"
      ],
      "metadata": {
        "id": "fSPjuvN8X-zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a copy of the DataFrame for standardization\n",
        "df_normstandard = scaled_df.copy()\n",
        "\n",
        "# Perform standardization on selected 3 columns\n",
        "df_normstandard[['Distance from Home', 'Distance from Last Transaction', 'Ratio to Median Purchase Price']] = scaler.fit_transform(df_normstandard[['Distance from Home', 'Distance from Last Transaction', 'Ratio to Median Purchase Price']])\n",
        "\n",
        "# Display the first 5 rows of the standardized DataFrame\n",
        "df_normstandard.head()"
      ],
      "metadata": {
        "id": "wBI0i5EdWsUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_exp = df_normstandard\n",
        "y_exp = X_exp.pop('Fraud')"
      ],
      "metadata": {
        "id": "c7eL2rPHXsvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_exp, X_test_exp, y_train_exp, y_test_exp = train_test_split(X_exp, y_exp, test_size=0.2)"
      ],
      "metadata": {
        "id": "oGBH3FIKXvrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to know the ccp_alpha... to run next code first"
      ],
      "metadata": {
        "id": "MqP4v_SYX1Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_exp = sklearn.tree.DecisionTreeClassifier(ccp_alpha=0.0, max_depth=5, min_samples_leaf=1, min_samples_split=7)\n",
        "model_exp.fit(X_train_exp, y_train_exp)"
      ],
      "metadata": {
        "id": "wHHt2Bc_Xz_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_exp.score(X_test_exp, y_test_exp)"
      ],
      "metadata": {
        "id": "n9lymKxaYcB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to change also next code"
      ],
      "metadata": {
        "id": "xukRAFg_YleE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a table to compare the accuracy value of model.score and model_exp.score\n",
        "\n",
        "model_score = model.score(X_test, y_test)\n",
        "model_exp_score = model_exp.score(X_test_exp, y_test_exp)\n",
        "\n",
        "table = pd.DataFrame({'Model': ['Decision Tree (Min Max Scaler)', 'Decision Tree (Standard Scaler)'],\n",
        "                     'Accuracy': [model_score, model_exp_score]})\n",
        "\n",
        "table\n"
      ],
      "metadata": {
        "id": "VJJ8Wg_gYg5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the experimental trial of using Standard Scaler as our scaling method, the accuracy seem to ..."
      ],
      "metadata": {
        "id": "nin3yN1xYtHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelining\n",
        "\n",
        "Since our data is preprocessed before entering into our ML model, we can create a pipeline. This allow us to generalize a dataset input from data preprocess to the ML model prediction and receive a output."
      ],
      "metadata": {
        "id": "lT7rsNTSY2Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create Pipeline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('model', DecisionTreeClassifier(random_state=161223, **clf.best_params_)),\n",
        "])"
      ],
      "metadata": {
        "id": "Bv-vjjCrY9HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline"
      ],
      "metadata": {
        "id": "EcHGQVuPZOmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Export Pipeline\n",
        "\n",
        "Now we can export our pipeline for deployment using joblib library."
      ],
      "metadata": {
        "id": "oeuG4Yi6ZQaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Fit the machine learning pipeline to the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained pipeline to a file using joblib named 'pipeline.pkl' and compression is applied\n",
        "joblib.dump(pipeline, 'pipeline.pkl', compress=True)"
      ],
      "metadata": {
        "id": "hfCjSXltZctl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pipeline\n",
        "\n",
        "We can load our pipeline and predict it with our new test data to see if it works as expected."
      ],
      "metadata": {
        "id": "U0HDwwofZh9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained machine learning pipeline\n",
        "pipeline_loaded = joblib.load('pipeline.pkl')\n",
        "\n",
        "# Calculate the accuracy score of the model\n",
        "pipeline_loaded.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "-N6KsghlZknT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As what we expect, our pipeline is able to preprocess the data correctly and give us a fairly accurate result."
      ],
      "metadata": {
        "id": "r1MGfgpgZpu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Deliverable\n",
        "Finally, you will need to present your findings and results in a clear and concise manner. Document it using Google Colab/Kaggle/Jupyter Notebook. You only\n",
        "submit the link to your notebook. Please make grant access for me to view and assess your work."
      ],
      "metadata": {
        "id": "se7UAWfAZt4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "The accuracy of the model is 0.9999930434782609. This shows that the accuracy of the trained model is impressively high, achieving a score of approximately 99.9993%.This high accuracy suggests that the model performs exceptionally well."
      ],
      "metadata": {
        "id": "lNk6u_3FZvI9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}